{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4eq5NfL3BlU"
      },
      "outputs": [],
      "source": [
        "# Install core libraries\n",
        "!pip install -q transformers sentence-transformers faiss-cpu langchain gradio chromadb qdrant-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive for access to your knowledge base and saved models/indexes\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VPL8X3Im34AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "fLD9Cz2H3MH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "HKUST_Course_Advisor_PATH = \"/content/drive/MyDrive/HKUST_Course_Advisor_Knowledge_Base\"\n",
        "\n",
        "files = [os.path.join(dp, f) for dp, _, fn in os.walk(HKUST_Course_Advisor_PATH ) for f in fn if f.endswith(\".md\")]\n",
        "docs = [doc for f in files for doc in TextLoader(f, encoding=\"utf-8\").load()]\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "texts = [chunk.page_content for chunk in chunks]\n",
        "print(f\"✅ {len(chunks)} chunks generated.\")"
      ],
      "metadata": {
        "id": "tlZX0Sg75oLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Embed all chunks\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "texts = [chunk.page_content for chunk in chunks]\n",
        "embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "# 2. PCA to 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings)\n",
        "\n",
        "# 3. Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, (x, y) in enumerate(reduced):\n",
        "    plt.scatter(x, y, s=10, c=\"navy\")\n",
        "    plt.text(x + 0.01, y + 0.01, str(i), fontsize=8)\n",
        "plt.title(\"PCA of Knowledge-Base Embeddings\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-TJZ-LKV7D4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain.vectorstores import FAISS as LangChainFAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# 1. Raw FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(dim)\n",
        "faiss_index.add(np.array(embeddings, dtype=\"float32\"))\n",
        "\n",
        "# 2. Prepare LangChain wrapper\n",
        "docs = [Document(page_content=t) for t in texts]\n",
        "docstore = InMemoryDocstore({str(i): docs[i] for i in range(len(docs))})\n",
        "id_map = {i: str(i) for i in range(len(docs))}\n",
        "embed_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore_faiss = LangChainFAISS(\n",
        "    index=faiss_index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=id_map,\n",
        "    embedding_function=embed_fn\n",
        ")\n",
        "\n",
        "print(\"✅ FAISS vectorstore ready.\")\n"
      ],
      "metadata": {
        "id": "9TACDzhSCe-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"things i need to know to make me decide\"\n",
        "results = vectorstore_faiss.similarity_search(query, k=3)\n",
        "\n",
        "print(\"Top-3 FAISS Results:\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n— Passage {i+1}:\\n{doc.page_content[:300]}…\")\n"
      ],
      "metadata": {
        "id": "Gl_eJtCsDBrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "\n",
        "# 1. Initialize persistent client (will create `chroma_data/` locally)\n",
        "client = chromadb.PersistentClient(\n",
        "    path=\"chroma_data\",\n",
        "    settings=Settings()\n",
        ")\n",
        "\n",
        "# 2. Create or load a collection named \"insurellm\"\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"HKUST\",\n",
        "    embedding_function=SentenceTransformerEmbeddingFunction(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 3. Upsert your chunks (ids + documents)\n",
        "ids = [str(i) for i in range(len(texts))]\n",
        "collection.upsert(\n",
        "    ids=ids,\n",
        "    documents=texts\n",
        ")\n",
        "\n",
        "# 4. Query for top-3 similar passages\n",
        "query = \"Describe HKUST\"\n",
        "resp = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=3\n",
        ")\n",
        "print(\"Top-3 ChromaDB Results:\")\n",
        "for doc in resp[\"documents\"][0]:\n",
        "    print(\"-\", doc[:200].replace(\"\\n\",\" \"), \"…\")\n"
      ],
      "metadata": {
        "id": "B9cZaT3CDQZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D PCA Visualization of Your Vector Store Embeddings\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 1. Extract FAISS index\n",
        "faiss_idx = vectorstore_faiss.index\n",
        "n = faiss_idx.ntotal\n",
        "\n",
        "# 2. Reconstruct all vectors\n",
        "emb_arr = np.vstack([faiss_idx.reconstruct(i) for i in range(n)])\n",
        "\n",
        "# 3. Reduce to 3D\n",
        "pca3 = PCA(n_components=3)\n",
        "embeddings_3d = pca3.fit_transform(emb_arr)\n",
        "\n",
        "# 4. Plot in 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "xs, ys, zs = embeddings_3d[:, 0], embeddings_3d[:, 1], embeddings_3d[:, 2]\n",
        "ax.scatter(xs, ys, zs)\n",
        "\n",
        "# Annotate each point with its index\n",
        "for i in range(n):\n",
        "    ax.text(xs[i], ys[i], zs[i], str(i), fontsize=8)\n",
        "\n",
        "ax.set_title(\"3D PCA of Vector Store Embeddings\")\n",
        "ax.set_xlabel(\"PC1\")\n",
        "ax.set_ylabel(\"PC2\")\n",
        "ax.set_zlabel(\"PC3\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AAhm8Nd09YTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# # Load your current notebook\n",
        "# notebook_path = '/content/HKUST_Vector_Database_And_Embeddings.ipynb'  # Update this path\n",
        "\n",
        "# # Read the notebook\n",
        "# with open(notebook_path, 'r') as f:\n",
        "#     notebook_data = json.load(f)\n",
        "\n",
        "# # Clear widget-related metadata\n",
        "# if 'metadata' in notebook_data:\n",
        "#     notebook_data['metadata'].pop('widgets', None)\n",
        "#     notebook_data['metadata'].pop('widget_state', None)\n",
        "#     notebook_data['metadata'].pop('widgets_state', None)\n",
        "\n",
        "# # Clear outputs and metadata from each cell\n",
        "# for cell in notebook_data.get('cells', []):\n",
        "#     if 'outputs' in cell:\n",
        "#         cell['outputs'] = []\n",
        "#     if 'metadata' in cell:\n",
        "#         cell['metadata'].pop('widgets', None)\n",
        "#         cell['metadata'].pop('widget_state', None)\n",
        "#         cell['metadata'].pop('widgets_state', None)\n",
        "\n",
        "# # Save the modified notebook\n",
        "# with open(notebook_path, 'w') as f:\n",
        "#     json.dump(notebook_data, f, indent=2)\n",
        "\n",
        "# print(\"All widget-related metadata removed successfully.\")"
      ],
      "metadata": {
        "id": "697o6qY2DJ51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rYJtNuHLGsXU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}